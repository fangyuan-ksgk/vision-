# vision+
Vision language model from scratch
1. Learn from blog: https://huggingface.co/blog/AviSoori1x/seemore-vision-language-model
2. Implementation of VLM from-scratch, using attention implementation from nanoGPT (Karparthy) and language decoder & vision encoder from the blog.
3. Stuck and confused about how to properly train on a toy case, which could be tested. Reading on opinion piece: https://huggingface.co/blog/Borise/law-vision-representation-in-mllms (I think Bohan Zhai is actually looking for collaborators at this time, the RAG idea of MLLM is interestingly resonating with my unifying tool with embedding idea)
4. The blog post is for the paper 'laws of MLLM', its codebase uses LlaVA's training pipeline here: https://github.com/haotian-liu/LLaVA/tree/main?tab=readme-ov-file#pretrain-feature-alignment, a twin implementation is here: https://github.com/SkunkworksAI/BakLLaVA
