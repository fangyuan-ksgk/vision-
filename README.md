# vision+
Vision language model from scratch
1. Learn from blog: https://huggingface.co/blog/AviSoori1x/seemore-vision-language-model
2. Implementation of VLM from-scratch, using attention implementation from nanoGPT (Karparthy) and language decoder & vision encoder from the blog. The naive implementation is inside the 'naive' folder. 
3. Learn from llava codebase: https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main | I'll focus on a minimal implementation of it first, with 1-1-1 on language decoder, multimodal encoder, and multimodal projector | In fact I will aim at building a tutorial on this, so that other people could easily follow it and learn to build their own VLM ....
