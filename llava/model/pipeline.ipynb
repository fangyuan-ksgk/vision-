{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On MacOS, decord package is not compatible with new python version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Model Arguments \n",
    "from config import ModelArguments\n",
    "\n",
    "config = ModelArguments()\n",
    "\n",
    "# Multimodal Encoder, Resampler, Projector\n",
    "from multimodal_encoder.builder import build_vision_tower\n",
    "from multimodal_resampler.builder import build_vision_resampler\n",
    "from multimodal_projector.builder import build_vision_projector\n",
    "\n",
    "vision_tower = build_vision_tower(config)\n",
    "vision_resampler = build_vision_resampler(config)\n",
    "vision_projector = build_vision_projector(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llava_llama import LlavaLlamaForCausalLM # Register the llava models into 'transformers'\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.for_model(\"llava_llama\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Dataset Construction for MultiModality\n",
    "from dataprocess import LazySupervisedDataset, DataCollatorForSupervisedDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from config import DataArguments \n",
    "from llava_llama import prep_llava_llama_tokenizer\n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path = \"data/mock.json\",\n",
    "    image_folder = \"data/\",\n",
    "    video_folder = \"data/\",\n",
    "    video_fps = 1,\n",
    "    frames_upbound = 0,\n",
    "    add_time_instruction = False,\n",
    "    force_sample = False,\n",
    "    default_fps = 10\n",
    ")\n",
    "\n",
    "tokenizer = prep_llava_llama_tokenizer(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "dataset = LazySupervisedDataset(data_args=data_args, tokenizer=tokenizer, image_processor=vision_tower.image_processor)\n",
    "collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, collate_fn=collator, batch_size=2, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train \n",
    "\n",
    "train(model, tokenizer, dataloader, use_lora=True) # This is easily the better choice (so many lower-level optimization happens here ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test-run with DOCCI training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from llava_llama import prep_llava_llama_tokenizer\n",
    "from dataprocess import prepare_docci_data, DataCollatorForSupervisedDataset\n",
    "\n",
    "tokenizer = prep_llava_llama_tokenizer(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "data_args = prepare_docci_data(\"data/docci_converted.json\", \"data/docci\")\n",
    "dataset = LazySupervisedDataset(data_args=data_args, tokenizer=tokenizer, image_processor=vision_tower.image_processor)\n",
    "collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, collate_fn=collator, batch_size=2, num_workers=1)\n",
    "\n",
    "from train import train \n",
    "\n",
    "train(model, tokenizer, dataloader, use_lora=True) # This is easily the better choice (so many lower-level optimization happens here ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make-shift generation pipeline \n",
    "\n",
    "test_loader = DataLoader(dataset, collate_fn=collator, batch_size=1)\n",
    "for data_batch in test_loader:\n",
    "    break\n",
    "\n",
    "input_ids, attention_mask, images, modalities, labels = data_batch[\"input_ids\"], data_batch[\"attention_mask\"], data_batch[\"images\"], data_batch[\"modalities\"], data_batch[\"labels\"]\n",
    "past_key_values, position_ids = None, None\n",
    "\n",
    "(_, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = model.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities)\n",
    "\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "output = super(LlamaForCausalLM, model).generate(\n",
    "    position_ids=position_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
