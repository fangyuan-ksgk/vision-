{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.99279356 0.00421071 0.00299575]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape\n",
    "# So the OPENAI CLIP model encode an image into a single 512 dim vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of the CLIPVisionTower class object\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor \n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"CLIP.png\")\n",
    "\n",
    "vision_tower_name = \"openai/clip-vit-base-patch32\"\n",
    "image_processor = CLIPImageProcessor.from_pretrained(vision_tower_name)\n",
    "vision_tower = CLIPVisionModel.from_pretrained(vision_tower_name)\n",
    "\n",
    "image = image_processor(img)\n",
    "# vision_tower(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer libray --> CLIPVisionModel & CLIPImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel, CLIPImageProcessor\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = Image.open(\"CLIP.png\")\n",
    "\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: openai/clip-vit-base-patch32\n",
      "openai/clip-vit-base-patch32 is already loaded, `load_model` called again, skipping.\n",
      "CLIPVisionTower initialized with vision tower: openai/clip-vit-base-patch32\n",
      "Select layer: -2\n",
      "Select feature: patch\n"
     ]
    }
   ],
   "source": [
    "from clip_encoder import CLIPVisionTower\n",
    "\n",
    "# Initialize CLIPVisionTower\n",
    "vision_tower_name = \"openai/clip-vit-base-patch32\"\n",
    "args = type('Args', (), {\n",
    "    'mm_vision_select_layer': -2,  # Typically the second to last layer\n",
    "    'mm_vision_select_feature': \"patch\"\n",
    "})()\n",
    "\n",
    "clip_vision_tower = CLIPVisionTower(vision_tower_name, args)\n",
    "\n",
    "# Load the model\n",
    "clip_vision_tower.load_model()\n",
    "\n",
    "# Now the CLIPVisionTower is initialized and ready to use\n",
    "print(f\"CLIPVisionTower initialized with vision tower: {clip_vision_tower.vision_tower_name}\")\n",
    "print(f\"Select layer: {clip_vision_tower.select_layer}\")\n",
    "print(f\"Select feature: {clip_vision_tower.select_feature}\")\n",
    "\n",
    "# Inference with the CLIPVisionTower class object\n",
    "from PIL import Image\n",
    "image = Image.open(\"CLIP.png\")\n",
    "inputs = clip_vision_tower.image_processor(image, return_tensors=\"pt\")\n",
    "tensors = inputs[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
