{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On MacOS, decord package is not compatible with new python version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Model Arguments \n",
    "from config import ModelArguments\n",
    "\n",
    "config = ModelArguments()\n",
    "\n",
    "# Multimodal Encoder, Resampler, Projector\n",
    "from multimodal_encoder.builder import build_vision_tower\n",
    "from multimodal_resampler.builder import build_vision_resampler\n",
    "from multimodal_projector.builder import build_vision_projector\n",
    "\n",
    "vision_tower = build_vision_tower(config)\n",
    "vision_resampler = build_vision_resampler(config)\n",
    "vision_projector = build_vision_projector(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llava_llama import LlavaLlamaForCausalLM # Register the llava models into 'transformers'\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.for_model(\"llava_llama\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Dataset Construction for MultiModality\n",
    "\n",
    "from dataprocess import LazySupervisedDataset\n",
    "from transformers import AutoTokenizer\n",
    "from config import DataArguments \n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path = \"data/mock.json\",\n",
    "    image_folder = \"data/\",\n",
    "    video_folder = \"data/\",\n",
    "    video_fps = 1,\n",
    "    frames_upbound = 0,\n",
    "    add_time_instruction = False,\n",
    "    force_sample = False,\n",
    "    default_fps = 10\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "dataset = LazySupervisedDataset(data_args=data_args, tokenizer=tokenizer, image_processor=vision_tower.image_processor)\n",
    "data_dict = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, now we try to propagate the data_dict into the model \n",
    "import torch \n",
    "\n",
    "# # Assuming you have already loaded the model\n",
    "# model = LlavaLlamaForCausalLM.from_pretrained(\"path_to_your_model\")\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Prepare the inputs\n",
    "input_ids = data_dict['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "labels = data_dict['labels'].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Prepare images, a list of per-patch-images tensors\n",
    "images = []\n",
    "if 'image' in data_dict:\n",
    "    batch_images = [item[0] for item in data_dict[\"image\"]]\n",
    "    batch_images = torch.cat(batch_images, dim=0)\n",
    "    images = [batch_images]\n",
    "\n",
    "# Create attention mask\n",
    "# attention_mask = (input_ids != model.config.pad_token_id).long()\n",
    "\n",
    "# # Forward pass\n",
    "# outputs = model(\n",
    "#     input_ids=input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     labels=labels,\n",
    "#     images=images,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
