{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On MacOS, decord package is not compatible with new python version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Model Arguments \n",
    "from config import ModelArguments\n",
    "\n",
    "config = ModelArguments()\n",
    "\n",
    "# Multimodal Encoder, Resampler, Projector\n",
    "from multimodal_encoder.builder import build_vision_tower\n",
    "from multimodal_resampler.builder import build_vision_resampler\n",
    "from multimodal_projector.builder import build_vision_projector\n",
    "\n",
    "vision_tower = build_vision_tower(config)\n",
    "vision_resampler = build_vision_resampler(config)\n",
    "vision_projector = build_vision_projector(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llava_llama import LlavaLlamaForCausalLM # Register the llava models into 'transformers'\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.for_model(\"llava_llama\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Dataset Construction for MultiModality\n",
    "\n",
    "from dataprocess import LazySupervisedDataset, DataCollatorForSupervisedDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from config import DataArguments \n",
    "from llava_llama import prep_llava_llama_tokenizer\n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path = \"data/mock.json\",\n",
    "    image_folder = \"data/\",\n",
    "    video_folder = \"data/\",\n",
    "    video_fps = 1,\n",
    "    frames_upbound = 0,\n",
    "    add_time_instruction = False,\n",
    "    force_sample = False,\n",
    "    default_fps = 10\n",
    ")\n",
    "\n",
    "tokenizer = prep_llava_llama_tokenizer(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "dataset = LazySupervisedDataset(data_args=data_args, tokenizer=tokenizer, image_processor=vision_tower.image_processor)\n",
    "collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "dataloader = DataLoader(dataset, collate_fn=collator, batch_size=2, num_workers=1)\n",
    "\n",
    "\n",
    "data_dict = dataset[0]\n",
    "\n",
    "# data_batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['image'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[[[-1.0623, -1.2521, -1.4127,  ..., -0.3178, -0.3032, -0.3178],\n",
       "            [-1.0623, -1.2083, -1.3543,  ..., -0.6390, -0.6244, -0.5368],\n",
       "            [-1.0623, -1.1645, -1.2813,  ..., -0.5514, -0.6390, -0.7412],\n",
       "            ...,\n",
       "            [ 1.2150,  1.2150,  1.2442,  ..., -1.0623, -0.9018, -0.7120],\n",
       "            [ 1.2150,  1.2296,  1.2296,  ..., -1.1353, -1.0039, -0.8580],\n",
       "            [ 1.2442,  1.2296,  1.2150,  ..., -1.1353, -1.0331, -0.9018]],\n",
       "  \n",
       "           [[-0.8666, -1.0617, -1.2268,  ..., -0.3864, -0.3864, -0.4164],\n",
       "            [-0.8666, -1.0167, -1.1668,  ..., -0.6715, -0.6715, -0.6115],\n",
       "            [-0.8516, -0.9717, -1.0918,  ..., -0.5815, -0.6715, -0.7766],\n",
       "            ...,\n",
       "            [ 1.2194,  1.2194,  1.2495,  ..., -1.0617, -0.9117, -0.7616],\n",
       "            [ 1.2194,  1.2344,  1.2344,  ..., -1.1368, -1.0167, -0.9117],\n",
       "            [ 1.2344,  1.2194,  1.2044,  ..., -1.1368, -1.0467, -0.9717]],\n",
       "  \n",
       "           [[-1.0821, -1.2385, -1.3665,  ..., -0.3568, -0.3568, -0.3853],\n",
       "            [-1.0821, -1.1958, -1.3238,  ..., -0.5986, -0.5986, -0.5133],\n",
       "            [-1.1105, -1.1958, -1.2954,  ..., -0.4706, -0.5559, -0.6555],\n",
       "            ...,\n",
       "            [ 1.1789,  1.1789,  1.2074,  ..., -0.8688, -0.7266, -0.5701],\n",
       "            [ 1.1932,  1.1932,  1.2074,  ..., -0.9114, -0.7977, -0.7123],\n",
       "            [ 1.2216,  1.2074,  1.1932,  ..., -0.9399, -0.8261, -0.7550]]]]),\n",
       "  tensor([[1, 0]])),\n",
       " (tensor([[[[-1.3251, -1.0039, -0.7996,  ..., -0.3178, -0.2886, -0.3178],\n",
       "            [-1.3689, -1.1061, -1.0039,  ..., -0.6682, -0.6390, -0.5952],\n",
       "            [-1.4857, -1.2813, -1.2375,  ..., -0.7412, -0.7850, -0.7704],\n",
       "            ...,\n",
       "            [ 1.2588,  1.2588,  1.2588,  ..., -0.5952, -0.5368, -0.5806],\n",
       "            [ 1.2734,  1.2588,  1.2588,  ..., -0.8726, -0.7558, -0.6828],\n",
       "            [ 1.2588,  1.2296,  1.2150,  ..., -1.0623, -0.9748, -0.9018]],\n",
       "  \n",
       "           [[-1.0918, -0.7616, -0.5515,  ..., -0.4764, -0.4464, -0.4764],\n",
       "            [-1.1218, -0.8516, -0.7466,  ..., -0.6565, -0.6415, -0.5965],\n",
       "            [-1.2568, -1.0467, -0.9867,  ..., -0.6865, -0.7316, -0.7166],\n",
       "            ...,\n",
       "            [ 1.2795,  1.2795,  1.2795,  ..., -0.6415, -0.6115, -0.6415],\n",
       "            [ 1.2945,  1.2795,  1.2795,  ..., -0.9267, -0.7916, -0.7166],\n",
       "            [ 1.2795,  1.2495,  1.2344,  ..., -1.0918, -1.0017, -0.9267]],\n",
       "  \n",
       "           [[-1.3665, -1.0821, -0.9825,  ..., -0.3426, -0.3284, -0.3568],\n",
       "            [-1.3665, -1.1389, -1.0963,  ..., -0.5559, -0.5417, -0.4990],\n",
       "            [-1.3949, -1.2385, -1.2669,  ..., -0.6128, -0.6412, -0.6270],\n",
       "            ...,\n",
       "            [ 1.2358,  1.2358,  1.2500,  ..., -0.4848, -0.4990, -0.5417],\n",
       "            [ 1.2643,  1.2500,  1.2358,  ..., -0.7123, -0.6555, -0.5986],\n",
       "            [ 1.2500,  1.2216,  1.2074,  ..., -0.8688, -0.8403, -0.7550]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.4565, -1.5003, -1.5441,  ..., -0.2740, -0.2740, -0.2448],\n",
       "            [-1.5149, -1.5587, -1.6025,  ..., -0.5222, -0.4784, -0.4200],\n",
       "            [-1.5295, -1.5733, -1.6463,  ..., -0.8142, -0.8142, -0.7558],\n",
       "            ...,\n",
       "            [ 1.4486,  1.4632,  1.4778,  ..., -0.7850, -0.7996, -0.8142],\n",
       "            [ 1.4486,  1.4632,  1.4778,  ..., -0.8872, -0.8872, -0.8872],\n",
       "            [ 1.4486,  1.4632,  1.4778,  ..., -0.9748, -0.9456, -0.9310]],\n",
       "  \n",
       "           [[-1.2568, -1.2869, -1.3319,  ..., -0.5065, -0.5065, -0.4764],\n",
       "            [-1.3019, -1.3469, -1.3919,  ..., -0.5515, -0.5065, -0.4464],\n",
       "            [-1.3169, -1.3769, -1.4369,  ..., -0.7766, -0.7616, -0.7016],\n",
       "            ...,\n",
       "            [ 1.5046,  1.5196,  1.5346,  ..., -0.9567, -0.9867, -1.0017],\n",
       "            [ 1.5046,  1.5196,  1.5346,  ..., -0.9567, -0.9567, -0.9567],\n",
       "            [ 1.5046,  1.5196,  1.5346,  ..., -0.9867, -0.9567, -0.9417]],\n",
       "  \n",
       "           [[-1.4518, -1.4518, -1.4518,  ..., -0.3853, -0.4279, -0.4279],\n",
       "            [-1.4660, -1.4660, -1.4802,  ..., -0.4564, -0.4279, -0.3995],\n",
       "            [-1.4518, -1.4660, -1.4802,  ..., -0.6839, -0.6697, -0.6270],\n",
       "            ...,\n",
       "            [ 1.4776,  1.4918,  1.5202,  ..., -0.8830, -0.8972, -0.9114],\n",
       "            [ 1.4776,  1.4918,  1.5202,  ..., -0.8403, -0.8545, -0.8545],\n",
       "            [ 1.4918,  1.5060,  1.5202,  ..., -0.8830, -0.8545, -0.8261]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.5003, -1.4273, -1.2229,  ..., -0.1718, -0.1718, -0.2156],\n",
       "            [-1.4127, -1.2959, -1.0039,  ..., -0.3470, -0.3032, -0.3032],\n",
       "            [-1.4127, -1.2959, -0.9456,  ..., -0.7266, -0.6974, -0.6974],\n",
       "            ...,\n",
       "            [ 1.4340,  1.4486,  1.4632,  ..., -0.7850, -0.8142, -0.8142],\n",
       "            [ 1.4194,  1.4486,  1.4632,  ..., -0.8872, -0.8872, -0.8872],\n",
       "            [ 1.4340,  1.4632,  1.4778,  ..., -0.9456, -0.9310, -0.9310]],\n",
       "  \n",
       "           [[-1.1368, -1.0767, -0.8516,  ..., -0.4464, -0.4314, -0.4764],\n",
       "            [-1.0467, -0.9417, -0.6115,  ..., -0.4464, -0.4014, -0.4014],\n",
       "            [-1.0467, -0.9267, -0.5365,  ..., -0.7916, -0.7466, -0.7466],\n",
       "            ...,\n",
       "            [ 1.5046,  1.5196,  1.5346,  ..., -0.9717, -0.9867, -1.0017],\n",
       "            [ 1.4896,  1.5196,  1.5346,  ..., -0.9567, -0.9567, -0.9567],\n",
       "            [ 1.5046,  1.5196,  1.5346,  ..., -0.9567, -0.9417, -0.9417]],\n",
       "  \n",
       "           [[-1.4802, -1.4233, -1.3238,  ..., -0.3568, -0.3426, -0.3853],\n",
       "            [-1.4660, -1.3522, -1.1532,  ..., -0.3284, -0.2857, -0.2857],\n",
       "            [-1.4518, -1.3522, -1.1532,  ..., -0.6128, -0.5844, -0.5844],\n",
       "            ...,\n",
       "            [ 1.5060,  1.5344,  1.5629,  ..., -0.8972, -0.9114, -0.9114],\n",
       "            [ 1.5060,  1.5344,  1.5487,  ..., -0.8545, -0.8545, -0.8545],\n",
       "            [ 1.5202,  1.5487,  1.5629,  ..., -0.8545, -0.8403, -0.8403]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.5879, -1.6025, -1.5879,  ..., -0.3616, -0.3324, -0.3324],\n",
       "            [-1.6317, -1.6317, -1.5879,  ..., -0.2156, -0.1864, -0.1718],\n",
       "            [-1.6317, -1.6171, -1.6171,  ..., -0.5222, -0.4054, -0.3032],\n",
       "            ...,\n",
       "            [ 1.2588,  1.2588,  1.2734,  ..., -0.8142, -0.7996, -0.7850],\n",
       "            [ 1.2734,  1.3026,  1.3318,  ..., -0.9164, -0.8726, -0.8726],\n",
       "            [ 1.3902,  1.4194,  1.4486,  ..., -1.1499, -1.1061, -1.0915]],\n",
       "  \n",
       "           [[-1.2718, -1.2718, -1.1968,  ..., -0.7616, -0.7466, -0.7766],\n",
       "            [-1.3169, -1.3169, -1.2418,  ..., -0.4614, -0.4464, -0.4464],\n",
       "            [-1.3319, -1.3169, -1.3169,  ..., -0.5515, -0.4764, -0.4014],\n",
       "            ...,\n",
       "            [ 1.2795,  1.3095,  1.3245,  ..., -0.9717, -0.9567, -0.9567],\n",
       "            [ 1.3245,  1.3695,  1.4145,  ..., -1.0167, -0.9717, -0.9567],\n",
       "            [ 1.4446,  1.4896,  1.5346,  ..., -1.1968, -1.1668, -1.1368]],\n",
       "  \n",
       "           [[-1.4802, -1.4802, -1.4802,  ..., -0.6697, -0.6555, -0.6697],\n",
       "            [-1.4802, -1.4802, -1.4802,  ..., -0.3426, -0.3284, -0.3142],\n",
       "            [-1.4802, -1.4660, -1.4802,  ..., -0.4422, -0.3568, -0.2715],\n",
       "            ...,\n",
       "            [ 1.2927,  1.3069,  1.3211,  ..., -0.8119, -0.8119, -0.8119],\n",
       "            [ 1.3211,  1.3496,  1.3922,  ..., -0.7977, -0.7692, -0.7692],\n",
       "            [ 1.4349,  1.4776,  1.4918,  ..., -0.9256, -0.8972, -0.8688]]]]),\n",
       "  tensor([[0, 1],\n",
       "          [0, 1],\n",
       "          [0, 1],\n",
       "          [0, 1]]))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_batch[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_img = sum([img.sum()==0 for img in images[0]])\n",
    "padded_modality = [mod.sum()==0 for mod in modalities[0]]\n",
    "assert padded_img == padded_modality, \"Modality mismatch with Image in padded number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, now we try to propagate the data_dict into the model \n",
    "import torch \n",
    "from constants import IGNORE_INDEX\n",
    "\n",
    "# Prepare the inputs\n",
    "input_ids = data_dict['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "labels = data_dict['labels'].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Prepare images, a list of per-patch-images tensors\n",
    "images = []\n",
    "if 'image' in data_dict:\n",
    "    batch_images = [item[0] for item in data_dict[\"image\"]]\n",
    "    batch_images = torch.cat(batch_images, dim=0)\n",
    "    images = [batch_images]\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    "    images=images,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
