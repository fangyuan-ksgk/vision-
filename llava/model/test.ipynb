{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On MacOS, decord package is not compatible with new python version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Model Arguments \n",
    "from config import ModelArguments\n",
    "\n",
    "config = ModelArguments()\n",
    "\n",
    "# Multimodal Encoder, Resampler, Projector\n",
    "from multimodal_encoder.builder import build_vision_tower\n",
    "from multimodal_resampler.builder import build_vision_resampler\n",
    "from multimodal_projector.builder import build_vision_projector\n",
    "\n",
    "vision_tower = build_vision_tower(config)\n",
    "vision_resampler = build_vision_resampler(config)\n",
    "vision_projector = build_vision_projector(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llava_llama import LlavaLlamaForCausalLM # Register the llava models into 'transformers'\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.for_model(\"llava_llama\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Key img_start_token is not a special token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllava_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prep_llava_llama_tokenizer\n\u001b[1;32m      8\u001b[0m data_args \u001b[38;5;241m=\u001b[39m DataArguments(\n\u001b[1;32m      9\u001b[0m     data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/mock.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     default_fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mprep_llava_llama_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m LazySupervisedDataset(data_args\u001b[38;5;241m=\u001b[39mdata_args, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, image_processor\u001b[38;5;241m=\u001b[39mvision_tower\u001b[38;5;241m.\u001b[39mimage_processor)\n\u001b[1;32m     21\u001b[0m collator \u001b[38;5;241m=\u001b[39m DataCollatorForSupervisedDataset(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/Implementation/vision-plus/llava/model/llava_llama.py:44\u001b[0m, in \u001b[0;36mprep_llava_llama_tokenizer\u001b[0;34m(model_name_or_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprep_llava_llama_tokenizer\u001b[39m(model_name_or_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AutoTokenizer:\n\u001b[1;32m     43\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpad_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbos_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|begin_of_text|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meos_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|eot_id|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_start_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|start_image_id|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_end_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|end_image_id|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvid_start_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|start_video_id|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvid_end_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|end_video_id|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madditional_special_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|start_header_id|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<|end_header_id|>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:950\u001b[0m, in \u001b[0;36mSpecialTokensMixin.add_special_tokens\u001b[0;34m(self, special_tokens_dict, replace_additional_special_tokens)\u001b[0m\n\u001b[1;32m    948\u001b[0m added_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m special_tokens_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 950\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSPECIAL_TOKENS_ATTRIBUTES, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a special token\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    953\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssigning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m key of the tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Key img_start_token is not a special token"
     ]
    }
   ],
   "source": [
    "# Dataset Construction for MultiModality\n",
    "\n",
    "from dataprocess import LazySupervisedDataset, DataCollatorForSupervisedDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from config import DataArguments \n",
    "from llava_llama import prep_llava_llama_tokenizer\n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path = \"data/mock.json\",\n",
    "    image_folder = \"data/\",\n",
    "    video_folder = \"data/\",\n",
    "    video_fps = 1,\n",
    "    frames_upbound = 0,\n",
    "    add_time_instruction = False,\n",
    "    force_sample = False,\n",
    "    default_fps = 10\n",
    ")\n",
    "\n",
    "tokenizer = prep_llava_llama_tokenizer(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "dataset = LazySupervisedDataset(data_args=data_args, tokenizer=tokenizer, image_processor=vision_tower.image_processor)\n",
    "collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "dataloader = DataLoader(dataset, collate_fn=collator, batch_size=2, num_workers=1)\n",
    "\n",
    "\n",
    "data_dict = dataset[0]\n",
    "\n",
    "data_batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.0623, -1.2521, -1.4127,  ..., -0.3178, -0.3032, -0.3178],\n",
       "           [-1.0623, -1.2083, -1.3543,  ..., -0.6390, -0.6244, -0.5368],\n",
       "           [-1.0623, -1.1645, -1.2813,  ..., -0.5514, -0.6390, -0.7412],\n",
       "           ...,\n",
       "           [ 1.2150,  1.2150,  1.2442,  ..., -1.0623, -0.9018, -0.7120],\n",
       "           [ 1.2150,  1.2296,  1.2296,  ..., -1.1353, -1.0039, -0.8580],\n",
       "           [ 1.2442,  1.2296,  1.2150,  ..., -1.1353, -1.0331, -0.9018]],\n",
       "\n",
       "          [[-0.8666, -1.0617, -1.2268,  ..., -0.3864, -0.3864, -0.4164],\n",
       "           [-0.8666, -1.0167, -1.1668,  ..., -0.6715, -0.6715, -0.6115],\n",
       "           [-0.8516, -0.9717, -1.0918,  ..., -0.5815, -0.6715, -0.7766],\n",
       "           ...,\n",
       "           [ 1.2194,  1.2194,  1.2495,  ..., -1.0617, -0.9117, -0.7616],\n",
       "           [ 1.2194,  1.2344,  1.2344,  ..., -1.1368, -1.0167, -0.9117],\n",
       "           [ 1.2344,  1.2194,  1.2044,  ..., -1.1368, -1.0467, -0.9717]],\n",
       "\n",
       "          [[-1.0821, -1.2385, -1.3665,  ..., -0.3568, -0.3568, -0.3853],\n",
       "           [-1.0821, -1.1958, -1.3238,  ..., -0.5986, -0.5986, -0.5133],\n",
       "           [-1.1105, -1.1958, -1.2954,  ..., -0.4706, -0.5559, -0.6555],\n",
       "           ...,\n",
       "           [ 1.1789,  1.1789,  1.2074,  ..., -0.8688, -0.7266, -0.5701],\n",
       "           [ 1.1932,  1.1932,  1.2074,  ..., -0.9114, -0.7977, -0.7123],\n",
       "           [ 1.2216,  1.2074,  1.1932,  ..., -0.9399, -0.8261, -0.7550]]],\n",
       "\n",
       "\n",
       "         [[[-1.3251, -1.0039, -0.7996,  ..., -0.3178, -0.2886, -0.3178],\n",
       "           [-1.3689, -1.1061, -1.0039,  ..., -0.6682, -0.6390, -0.5952],\n",
       "           [-1.4857, -1.2813, -1.2375,  ..., -0.7412, -0.7850, -0.7704],\n",
       "           ...,\n",
       "           [ 1.2588,  1.2588,  1.2588,  ..., -0.5952, -0.5368, -0.5806],\n",
       "           [ 1.2734,  1.2588,  1.2588,  ..., -0.8726, -0.7558, -0.6828],\n",
       "           [ 1.2588,  1.2296,  1.2150,  ..., -1.0623, -0.9748, -0.9018]],\n",
       "\n",
       "          [[-1.0918, -0.7616, -0.5515,  ..., -0.4764, -0.4464, -0.4764],\n",
       "           [-1.1218, -0.8516, -0.7466,  ..., -0.6565, -0.6415, -0.5965],\n",
       "           [-1.2568, -1.0467, -0.9867,  ..., -0.6865, -0.7316, -0.7166],\n",
       "           ...,\n",
       "           [ 1.2795,  1.2795,  1.2795,  ..., -0.6415, -0.6115, -0.6415],\n",
       "           [ 1.2945,  1.2795,  1.2795,  ..., -0.9267, -0.7916, -0.7166],\n",
       "           [ 1.2795,  1.2495,  1.2344,  ..., -1.0918, -1.0017, -0.9267]],\n",
       "\n",
       "          [[-1.3665, -1.0821, -0.9825,  ..., -0.3426, -0.3284, -0.3568],\n",
       "           [-1.3665, -1.1389, -1.0963,  ..., -0.5559, -0.5417, -0.4990],\n",
       "           [-1.3949, -1.2385, -1.2669,  ..., -0.6128, -0.6412, -0.6270],\n",
       "           ...,\n",
       "           [ 1.2358,  1.2358,  1.2500,  ..., -0.4848, -0.4990, -0.5417],\n",
       "           [ 1.2643,  1.2500,  1.2358,  ..., -0.7123, -0.6555, -0.5986],\n",
       "           [ 1.2500,  1.2216,  1.2074,  ..., -0.8688, -0.8403, -0.7550]]],\n",
       "\n",
       "\n",
       "         [[[-1.4565, -1.5003, -1.5441,  ..., -0.2740, -0.2740, -0.2448],\n",
       "           [-1.5149, -1.5587, -1.6025,  ..., -0.5222, -0.4784, -0.4200],\n",
       "           [-1.5295, -1.5733, -1.6463,  ..., -0.8142, -0.8142, -0.7558],\n",
       "           ...,\n",
       "           [ 1.4486,  1.4632,  1.4778,  ..., -0.7850, -0.7996, -0.8142],\n",
       "           [ 1.4486,  1.4632,  1.4778,  ..., -0.8872, -0.8872, -0.8872],\n",
       "           [ 1.4486,  1.4632,  1.4778,  ..., -0.9748, -0.9456, -0.9310]],\n",
       "\n",
       "          [[-1.2568, -1.2869, -1.3319,  ..., -0.5065, -0.5065, -0.4764],\n",
       "           [-1.3019, -1.3469, -1.3919,  ..., -0.5515, -0.5065, -0.4464],\n",
       "           [-1.3169, -1.3769, -1.4369,  ..., -0.7766, -0.7616, -0.7016],\n",
       "           ...,\n",
       "           [ 1.5046,  1.5196,  1.5346,  ..., -0.9567, -0.9867, -1.0017],\n",
       "           [ 1.5046,  1.5196,  1.5346,  ..., -0.9567, -0.9567, -0.9567],\n",
       "           [ 1.5046,  1.5196,  1.5346,  ..., -0.9867, -0.9567, -0.9417]],\n",
       "\n",
       "          [[-1.4518, -1.4518, -1.4518,  ..., -0.3853, -0.4279, -0.4279],\n",
       "           [-1.4660, -1.4660, -1.4802,  ..., -0.4564, -0.4279, -0.3995],\n",
       "           [-1.4518, -1.4660, -1.4802,  ..., -0.6839, -0.6697, -0.6270],\n",
       "           ...,\n",
       "           [ 1.4776,  1.4918,  1.5202,  ..., -0.8830, -0.8972, -0.9114],\n",
       "           [ 1.4776,  1.4918,  1.5202,  ..., -0.8403, -0.8545, -0.8545],\n",
       "           [ 1.4918,  1.5060,  1.5202,  ..., -0.8830, -0.8545, -0.8261]]],\n",
       "\n",
       "\n",
       "         [[[-1.5003, -1.4273, -1.2229,  ..., -0.1718, -0.1718, -0.2156],\n",
       "           [-1.4127, -1.2959, -1.0039,  ..., -0.3470, -0.3032, -0.3032],\n",
       "           [-1.4127, -1.2959, -0.9456,  ..., -0.7266, -0.6974, -0.6974],\n",
       "           ...,\n",
       "           [ 1.4340,  1.4486,  1.4632,  ..., -0.7850, -0.8142, -0.8142],\n",
       "           [ 1.4194,  1.4486,  1.4632,  ..., -0.8872, -0.8872, -0.8872],\n",
       "           [ 1.4340,  1.4632,  1.4778,  ..., -0.9456, -0.9310, -0.9310]],\n",
       "\n",
       "          [[-1.1368, -1.0767, -0.8516,  ..., -0.4464, -0.4314, -0.4764],\n",
       "           [-1.0467, -0.9417, -0.6115,  ..., -0.4464, -0.4014, -0.4014],\n",
       "           [-1.0467, -0.9267, -0.5365,  ..., -0.7916, -0.7466, -0.7466],\n",
       "           ...,\n",
       "           [ 1.5046,  1.5196,  1.5346,  ..., -0.9717, -0.9867, -1.0017],\n",
       "           [ 1.4896,  1.5196,  1.5346,  ..., -0.9567, -0.9567, -0.9567],\n",
       "           [ 1.5046,  1.5196,  1.5346,  ..., -0.9567, -0.9417, -0.9417]],\n",
       "\n",
       "          [[-1.4802, -1.4233, -1.3238,  ..., -0.3568, -0.3426, -0.3853],\n",
       "           [-1.4660, -1.3522, -1.1532,  ..., -0.3284, -0.2857, -0.2857],\n",
       "           [-1.4518, -1.3522, -1.1532,  ..., -0.6128, -0.5844, -0.5844],\n",
       "           ...,\n",
       "           [ 1.5060,  1.5344,  1.5629,  ..., -0.8972, -0.9114, -0.9114],\n",
       "           [ 1.5060,  1.5344,  1.5487,  ..., -0.8545, -0.8545, -0.8545],\n",
       "           [ 1.5202,  1.5487,  1.5629,  ..., -0.8545, -0.8403, -0.8403]]],\n",
       "\n",
       "\n",
       "         [[[-1.5879, -1.6025, -1.5879,  ..., -0.3616, -0.3324, -0.3324],\n",
       "           [-1.6317, -1.6317, -1.5879,  ..., -0.2156, -0.1864, -0.1718],\n",
       "           [-1.6317, -1.6171, -1.6171,  ..., -0.5222, -0.4054, -0.3032],\n",
       "           ...,\n",
       "           [ 1.2588,  1.2588,  1.2734,  ..., -0.8142, -0.7996, -0.7850],\n",
       "           [ 1.2734,  1.3026,  1.3318,  ..., -0.9164, -0.8726, -0.8726],\n",
       "           [ 1.3902,  1.4194,  1.4486,  ..., -1.1499, -1.1061, -1.0915]],\n",
       "\n",
       "          [[-1.2718, -1.2718, -1.1968,  ..., -0.7616, -0.7466, -0.7766],\n",
       "           [-1.3169, -1.3169, -1.2418,  ..., -0.4614, -0.4464, -0.4464],\n",
       "           [-1.3319, -1.3169, -1.3169,  ..., -0.5515, -0.4764, -0.4014],\n",
       "           ...,\n",
       "           [ 1.2795,  1.3095,  1.3245,  ..., -0.9717, -0.9567, -0.9567],\n",
       "           [ 1.3245,  1.3695,  1.4145,  ..., -1.0167, -0.9717, -0.9567],\n",
       "           [ 1.4446,  1.4896,  1.5346,  ..., -1.1968, -1.1668, -1.1368]],\n",
       "\n",
       "          [[-1.4802, -1.4802, -1.4802,  ..., -0.6697, -0.6555, -0.6697],\n",
       "           [-1.4802, -1.4802, -1.4802,  ..., -0.3426, -0.3284, -0.3142],\n",
       "           [-1.4802, -1.4660, -1.4802,  ..., -0.4422, -0.3568, -0.2715],\n",
       "           ...,\n",
       "           [ 1.2927,  1.3069,  1.3211,  ..., -0.8119, -0.8119, -0.8119],\n",
       "           [ 1.3211,  1.3496,  1.3922,  ..., -0.7977, -0.7692, -0.7692],\n",
       "           [ 1.4349,  1.4776,  1.4918,  ..., -0.9256, -0.8972, -0.8688]]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right, now we try to propagate the data_dict into the model \n",
    "import torch \n",
    "from constants import IGNORE_INDEX\n",
    "\n",
    "# Prepare the inputs\n",
    "input_ids = data_dict['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "labels = data_dict['labels'].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Prepare images, a list of per-patch-images tensors\n",
    "images = []\n",
    "if 'image' in data_dict:\n",
    "    batch_images = [item[0] for item in data_dict[\"image\"]]\n",
    "    batch_images = torch.cat(batch_images, dim=0)\n",
    "    images = [batch_images]\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    "    images=images,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
