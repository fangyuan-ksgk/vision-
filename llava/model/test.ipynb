{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On MacOS, decord package is not compatible with new python version.\n",
      "Loading vision tower: openai/clip-vit-base-patch32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Model Arguments \n",
    "from config import ModelArguments\n",
    "\n",
    "config = ModelArguments()\n",
    "\n",
    "# Multimodal Encoder, Resampler, Projector\n",
    "from multimodal_encoder.builder import build_vision_tower\n",
    "from multimodal_resampler.builder import build_vision_resampler\n",
    "from multimodal_projector.builder import build_vision_projector\n",
    "\n",
    "vision_tower = build_vision_tower(config)\n",
    "vision_resampler = build_vision_resampler(config)\n",
    "vision_projector = build_vision_projector(config)\n",
    "\n",
    "# Llava Model :: inherit from transformers.PreTrainedModel class unique initialization with config\n",
    "# from llava_llama import LlavaConfig, LlavaLlamaModel\n",
    "\n",
    "# llava_config = LlavaConfig()\n",
    "# llava_llama_model = LlavaLlamaModel(llava_config) # took around 2 min to initialize already (time to move to RunPod?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llava_llama import LlavaLlamaForCausalLM\n",
    "\n",
    "# LlavaLlamaForCausalLM(llava_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_model.llava_llama import LlavaLlamaForCausalLM # Register the llava models into 'transformers'\n",
    "# from llava_arch import LlavaMetaModel\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.for_model(\"llava_llama\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "container = av.open(\"vid.mp4\")\n",
    "for frame in container.decode(video=0):\n",
    "    img_pil = frame.to_image()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from dataprocess import LazySupervisedDataset\n",
    "from transformers import AutoTokenizer\n",
    "from config import DataArguments \n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path = \"mock.json\",\n",
    "    image_folder = \"\",\n",
    "    video_folder = \"\",\n",
    "    video_fps = 1,\n",
    "    frames_upbound = 0,\n",
    "    add_time_instruction = False,\n",
    "    force_sample = False,\n",
    "    default_fps = 10\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "dataset = LazySupervisedDataset(data_args=data_args, tokenizer=tokenizer, image_processor=vision_tower.image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = dataset.data[0]\n",
    "# 'image' in source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([128000, 128006,   9125, 128007,    271,   2675,    527,    264,  11190,\n",
       "           4221,    323,  11376,  18328,     13,   1472,    527,   3025,    311,\n",
       "           3619,    279,   9302,   2262,    430,    279,   1217,   5825,     11,\n",
       "            323,   7945,    279,   1217,    449,    264,   8205,    315,   9256,\n",
       "           1701,   5933,   4221,     13, 128009, 128006,    882, 128007,    271,\n",
       "           -200,    198,   3923,    649,    499,   1518,    304,    420,   2217,\n",
       "             30, 128009, 128000, 128006,  78191, 128007,    271, 128000, 128009,\n",
       "         128006,    882, 128007,    271,  11787,   1070,    904,   1274,    304,\n",
       "            279,   2217,     30, 128009, 128000, 128006,  78191, 128007,    271,\n",
       "         128000, 128009]),\n",
       " 'labels': tensor([128000, 128006,   -100, 128007,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100, 128009, 128006,   -100, 128007,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100, 128009, 128000, 128006,   -100, 128007,   -100, 128000, 128009,\n",
       "         128006,   -100, 128007,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100, 128009, 128000, 128006,   -100, 128007,   -100,\n",
       "         128000, 128009]),\n",
       " 'image': tensor([[[-0.8580, -0.9456, -1.0331,  ..., -1.6901, -1.6901, -1.6901],\n",
       "          [-0.5806, -0.6828, -0.7850,  ..., -1.6901, -1.6901, -1.6755],\n",
       "          [-0.3032, -0.3762, -0.4492,  ..., -1.6901, -1.6901, -1.6755],\n",
       "          ...,\n",
       "          [-1.6463, -1.6463, -1.6463,  ..., -1.6609, -1.6609, -1.6463],\n",
       "          [-1.6463, -1.6463, -1.6463,  ..., -1.6463, -1.6609, -1.6463],\n",
       "          [-1.6463, -1.6463, -1.6463,  ..., -1.6463, -1.6609, -1.6463]],\n",
       " \n",
       "         [[-1.0918, -1.1518, -1.2118,  ..., -1.5870, -1.5870, -1.5870],\n",
       "          [-0.8516, -0.9417, -1.0167,  ..., -1.5870, -1.5870, -1.5720],\n",
       "          [-0.6115, -0.6715, -0.7466,  ..., -1.5870, -1.5870, -1.5720],\n",
       "          ...,\n",
       "          [-1.5870, -1.5870, -1.5870,  ..., -1.6170, -1.6170, -1.6020],\n",
       "          [-1.5870, -1.5870, -1.5870,  ..., -1.6020, -1.6170, -1.6020],\n",
       "          [-1.5870, -1.5870, -1.5870,  ..., -1.6020, -1.6170, -1.6020]],\n",
       " \n",
       "         [[ 0.2831,  0.1835,  0.0698,  ..., -1.1532, -1.1532, -1.1532],\n",
       "          [ 0.5959,  0.4821,  0.3684,  ..., -1.1532, -1.1532, -1.1389],\n",
       "          [ 0.8945,  0.8234,  0.7523,  ..., -1.1532, -1.1532, -1.1389],\n",
       "          ...,\n",
       "          [-1.0394, -1.0394, -1.0394,  ..., -1.1958, -1.2100, -1.1958],\n",
       "          [-1.0394, -1.0394, -1.0394,  ..., -1.2100, -1.2243, -1.2100],\n",
       "          [-1.0394, -1.0394, -1.0394,  ..., -1.2243, -1.2385, -1.2243]]]),\n",
       " 'id': 'image_1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
