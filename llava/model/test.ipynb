{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On MacOS, decord package is not compatible with new python version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Model Arguments \n",
    "from config import ModelArguments\n",
    "\n",
    "config = ModelArguments()\n",
    "\n",
    "# Multimodal Encoder, Resampler, Projector\n",
    "from multimodal_encoder.builder import build_vision_tower\n",
    "from multimodal_resampler.builder import build_vision_resampler\n",
    "from multimodal_projector.builder import build_vision_projector\n",
    "\n",
    "vision_tower = build_vision_tower(config)\n",
    "vision_resampler = build_vision_resampler(config)\n",
    "vision_projector = build_vision_projector(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from llava_llama import LlavaLlamaForCausalLM # Register the llava models into 'transformers'\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = AutoConfig.for_model(\"llava_llama\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ad68e2f3004f299993da0232051a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0d336f31224bda8f0504394cc3707b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d610a7e86cc427baf19618711817206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Dataset Construction for MultiModality\n",
    "\n",
    "from dataprocess import LazySupervisedDataset\n",
    "from config import DataArguments \n",
    "from llava_llama import prep_llava_llama_tokenizer\n",
    "\n",
    "data_args = DataArguments(\n",
    "    data_path = \"data/mock.json\",\n",
    "    image_folder = \"data/\",\n",
    "    video_folder = \"data/\",\n",
    "    video_fps = 1,\n",
    "    frames_upbound = 0,\n",
    "    add_time_instruction = False,\n",
    "    force_sample = False,\n",
    "    default_fps = 10\n",
    ")\n",
    "\n",
    "tokenizer = prep_llava_llama_tokenizer(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "dataset = LazySupervisedDataset(data_args=data_args, tokenizer=tokenizer, image_processor=vision_tower.image_processor)\n",
    "data_dict = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "           220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n",
       "           264,  11190,   4221,    323,  11376,  18328,     13,   1472,    527,\n",
       "          3025,    311,   3619,    279,   9302,   2262,    430,    279,   1217,\n",
       "          5825,     11,    323,   7945,    279,   1217,    449,    264,   8205,\n",
       "           315,   9256,   1701,   5933,   4221,     13, 128009, 128006,   9125,\n",
       "        128007,    271,  38766,   1303,  33025,   2696,     25,   6790,    220,\n",
       "          2366,     18,    198,  15724,   2696,     25,    220,   1627,  10263,\n",
       "           220,   2366,     19,    271, 128009, 128006,    882, 128007,    271,\n",
       "          -200,    198,   3923,    649,    499,   1518,    304,    420,   2217,\n",
       "            30, 128009, 128000, 128006,   9125, 128007,    271,  38766,   1303,\n",
       "         33025,   2696,     25,   6790,    220,   2366,     18,    198,  15724,\n",
       "          2696,     25,    220,   1627,  10263,    220,   2366,     19,    271,\n",
       "        128009, 128006,  78191, 128007,    271, 128000,    791,   2217,   5039,\n",
       "           264,   6366,  44084,    927,    264,  11573,     13,    578,  13180,\n",
       "           374,  24937,    449,  34076,  19087,    323,  18718,  82757,     11,\n",
       "         42852,   1022,    279,  19858,  18435,   3090,     13,    763,    279,\n",
       "         40405,     11,   1070,    596,    264,  57827,    315,    264,  33552,\n",
       "          5021,     11,   7999,    264,  35148,   2733,    311,    279,   6237,\n",
       "            13, 128009, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
       "          2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
       "            25,    220,   1627,  10263,    220,   2366,     19,    271, 128009,\n",
       "        128006,    882, 128007,    271,   -200,   -200,   -200,   -200,    198,\n",
       "         75885,   1148,    596,  12765,    304,    420,   2835,     13, 128009,\n",
       "        128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "           220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
       "         78191, 128007,    271, 128000,    791,   2835,   5039,    264,  13326,\n",
       "          3363,   8761,   2391,  13270,   6596,     13,   2684,    527,  12387,\n",
       "          9515,    323,  34480,   7366,  14297,   1555,   9629,     13,  19878,\n",
       "           478,  36268,    527,  27736,    520,   5425,  19599,     82,    323,\n",
       "         11689,    389,  82835,     13,    578,   6237,    374,  14595,    315,\n",
       "           264,  90256,  16036,   4676,   2391,    264,  47678,   6693,    477,\n",
       "         11714,     13, 128009])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaLlamaTokenizer(LlamaTokenizer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.add_special_tokens({\n",
    "            'pad_token': '[PAD]',\n",
    "            'bos_token': '<|begin_of_text|>',\n",
    "            'eos_token': '<|eot_id|>',\n",
    "            'additional_special_tokens': ['<|start_header_id|>', '<|end_header_id|>']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nhahaha<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nhehehe<|eot_id|>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = [{\"role\": \"user\", \"content\": \"hahaha\"}, {\"role\": \"assistant\", \"content\": \"hehehe\"}]\n",
    "tokenizer.apply_chat_template(msg, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 128006, 128007, 128009]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = [\"<|begin_of_text|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"]\n",
    "special_tokens_idx = [tokenizer.convert_tokens_to_ids(tok) for tok in special_tokens]\n",
    "special_tokens_idx\n",
    "\n",
    "# this is the standard manuver required for training llama class models ...\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m!=\u001b[39m IGNORE_INDEX)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(\n\u001b[1;32m     25\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     26\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     27\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m     28\u001b[0m     images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m     29\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Right, now we try to propagate the data_dict into the model \n",
    "import torch \n",
    "from constants import IGNORE_INDEX\n",
    "\n",
    "# # Assuming you have already loaded the model\n",
    "# model = LlavaLlamaForCausalLM.from_pretrained(\"path_to_your_model\")\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Prepare the inputs\n",
    "input_ids = data_dict['input_ids'].unsqueeze(0)  # Add batch dimension\n",
    "labels = data_dict['labels'].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Prepare images, a list of per-patch-images tensors\n",
    "images = []\n",
    "if 'image' in data_dict:\n",
    "    batch_images = [item[0] for item in data_dict[\"image\"]]\n",
    "    batch_images = torch.cat(batch_images, dim=0)\n",
    "    images = [batch_images]\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = (labels != IGNORE_INDEX).long()\n",
    "\n",
    "# Forward pass\n",
    "# outputs = model(\n",
    "#     input_ids=input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     labels=labels,\n",
    "#     images=images,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Tracing \n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, images=images) # Error occurs\n",
    "model.prepare_inputs_labels_for_multimodal(input_ids, None, attention_mask, None, labels, images) # Same Error Propagated here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
